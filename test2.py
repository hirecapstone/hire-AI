import firebase_admin
from firebase_admin import credentials, storage
import cv2
import mediapipe as mp
import numpy as np
import os
import time
import sys

if len(sys.argv) < 2:
    print("âŒ mp4 íŒŒì¼ëª…ì„ ì¸ìë¡œ ë„˜ê²¨ì£¼ì„¸ìš”.")
    sys.exit(1)

video_path = sys.argv[1]

# ===== ë¶„ì„ í•¨ìˆ˜ =====
def extract_landmarks(results, face=False, pose=False):
    landmarks = []
    if face and results.multi_face_landmarks:
        for lm in results.multi_face_landmarks[0].landmark:
            landmarks.append([lm.x, lm.y, lm.z])
    elif pose and results.pose_landmarks:
        for lm in results.pose_landmarks.landmark:
            landmarks.append([lm.x, lm.y, lm.z])
    return landmarks

def calculate_smile_score(landmarks):
    try:
        left = np.array(landmarks[61])
        right = np.array(landmarks[291])
        mid = np.array(landmarks[13])  # ìœ—ì…ìˆ  ì¤‘ì•™

        mouth_width = np.linalg.norm(right[:2] - left[:2])
        mouth_height = abs(mid[1] - (left[1] + right[1]) / 2)
        slope = (left[1] + right[1]) / 2 - mid[1]

        return (mouth_height + slope) / mouth_width
    except:
        return None

def calculate_eye_diff(landmarks):
    if len(landmarks) < 468:
        return None
    return abs(landmarks[33][0] - landmarks[263][0])

def calculate_posture_metrics(pose_lm):
    if len(pose_lm) < 13:
        return None, None
    ls = pose_lm[11]
    rs = pose_lm[12]
    shoulder_y_diff = ls[1] - rs[1]
    shoulder_z = (ls[2] + rs[2]) / 2
    return shoulder_y_diff, shoulder_z

def detect_iris_direction_refined(landmarks):
    try:
        left_iris_x = landmarks[468][0]
        left_inner = landmarks[133][0]
        left_outer = landmarks[33][0]
        left_width = abs(left_outer - left_inner)
        left_offset_ratio = abs(left_iris_x - (left_inner + left_outer) / 2) / left_width

        right_iris_x = landmarks[473][0]
        right_inner = landmarks[362][0]
        right_outer = landmarks[263][0]
        right_width = abs(right_outer - right_inner)
        right_offset_ratio = abs(right_iris_x - (right_inner + right_outer) / 2) / right_width

        avg_offset_ratio = (left_offset_ratio + right_offset_ratio) / 2
        return avg_offset_ratio < 0.01
    except:
        return True

# ===== ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ =====
def analyze_video(video_path):
    mp_pose = mp.solutions.pose
    mp_face_mesh = mp.solutions.face_mesh
    pose = mp_pose.Pose()
    face = mp_face_mesh.FaceMesh(refine_landmarks=True)

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = frame_count / fps
    print(f"ğŸ FPS: {fps}, ì „ì²´ í”„ë ˆì„: {frame_count}, ì˜ìƒ ê¸¸ì´: {duration:.2f}ì´ˆ")

    face_buffer, pose_buffer = [], []
    smile_scores, eye_diffs, posture_ys, posture_zs = [], [], [], []

    face_intervals, posture_intervals, not_looking_intervals = [], [], []
    face_state, face_start = "ì¤‘ë¦½", None
    posture_state, posture_start = "ì •ìì„¸ì…ë‹ˆë‹¤", None
    posture_change_count = 0
    not_looking_count = 0

    current_sec, frame_idx = 0, 0
    start_time = time.time()

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        face_result = face.process(image_rgb)
        pose_result = pose.process(image_rgb)
        face_lm = extract_landmarks(face_result, face=True)
        pose_lm = extract_landmarks(pose_result, pose=True)
        face_buffer.append(face_lm)
        pose_buffer.append(pose_lm)

        frame_idx += 1
        if frame_idx >= (current_sec + 1) * fps:
            face_valid = [np.array(f) for f in face_buffer if len(f) > 0]
            pose_valid = [np.array(p) for p in pose_buffer if len(p) > 0]
            face_avg = np.mean(face_valid, axis=0) if face_valid else None
            pose_avg = np.mean(pose_valid, axis=0) if pose_valid else None

            if current_sec < 5:
                if face_avg is not None:
                    smile = calculate_smile_score(face_avg)
                    eye = calculate_eye_diff(face_avg)
                    if smile is not None:
                        smile_scores.append(smile)
                    if eye is not None:
                        eye_diffs.append(eye)
                if pose_avg is not None:
                    y_diff, z_avg = calculate_posture_metrics(pose_avg)
                    if y_diff is not None:
                        posture_ys.append(y_diff)
                    if z_avg is not None:
                        posture_zs.append(z_avg)
            else:
                if face_avg is not None:
                    smile_score = calculate_smile_score(face_avg)
                    eye_diff = calculate_eye_diff(face_avg)
                    iris_check = detect_iris_direction_refined(face_avg)
                    baseline_smile = np.mean(smile_scores) if smile_scores else 0
                    baseline_eye = np.mean(eye_diffs) if eye_diffs else 0

                    # ë””ë²„ê¹… ì¶œë ¥
                    print(f"[DEBUG] ì´ˆ:{current_sec}, smile_score:{smile_score:.4f}, baseline:{baseline_smile:.4f}, diff:{smile_score - baseline_smile:.4f}")

                    new_face_state = "ì›ƒìŒ" if smile_score - baseline_smile > 0.11 else "ì¤‘ë¦½"
                    if new_face_state != face_state:
                        if face_state != "ì¤‘ë¦½" and face_start is not None:
                            face_intervals.append((face_start, current_sec, face_state))
                        face_start = current_sec if new_face_state != "ì¤‘ë¦½" else None
                        face_state = new_face_state

                    if eye_diff - baseline_eye > 0.02 or not iris_check:
                        not_looking_intervals.append(current_sec)
                        not_looking_count += 1

                if pose_avg is not None:
                    y_diff, z_avg = calculate_posture_metrics(pose_avg)
                    base_y = np.mean(posture_ys) if posture_ys else 0
                    base_z = np.mean(posture_zs) if posture_zs else 0
                    dy = y_diff - base_y
                    dz = z_avg - base_z

                    if dy > 0.04:
                        new_posture = "ì™¼ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì¡ŒìŠµë‹ˆë‹¤"
                    elif dy < -0.04:
                        new_posture = "ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì¡ŒìŠµë‹ˆë‹¤"
                    elif dz < -0.1:
                        new_posture = "ëª¸ì´ ì•ìœ¼ë¡œ ìˆ™ì—¬ì¡ŒìŠµë‹ˆë‹¤"
                    elif dz > 0.1:
                        new_posture = "ëª¸ì´ ë’¤ë¡œ ì –í˜€ì¡ŒìŠµë‹ˆë‹¤"
                    else:
                        new_posture = "ì •ìì„¸ì…ë‹ˆë‹¤"

                    if new_posture != posture_state:
                        if posture_state != "ì •ìì„¸ì…ë‹ˆë‹¤" and posture_start is not None:
                            posture_intervals.append((posture_start, current_sec, posture_state))
                            posture_change_count += 1
                        posture_start = current_sec if new_posture != "ì •ìì„¸ì…ë‹ˆë‹¤" else None
                        posture_state = new_posture

            face_buffer.clear()
            pose_buffer.clear()
            current_sec += 1

    cap.release()
    if face_state != "ì¤‘ë¦½" and face_start is not None:
        face_intervals.append((face_start, current_sec, face_state))
    if posture_state != "ì •ìì„¸ì…ë‹ˆë‹¤" and posture_start is not None:
        posture_intervals.append((posture_start, current_sec, posture_state))
        posture_change_count += 1

    end_time = time.time()

    # ===== ê²°ê³¼ ì¶œë ¥ =====
    print("\nğŸ™‚ ì–¼êµ´ í‘œì • ë¶„ì„ ê²°ê³¼:")
    if face_intervals:
        for s, e, state in face_intervals:
            print(f" - {s}ì´ˆ ~ {e}ì´ˆ ì‚¬ì´ì— {state} í‘œì •ì„ ì§€ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(" - ê°ì§€ëœ í‘œì • ë³€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\nğŸ•º ìì„¸ ë¶„ì„ ê²°ê³¼:")
    if posture_intervals:
        for s, e, state in posture_intervals:
            print(f" - {s}ì´ˆ ~ {e}ì´ˆ ì‚¬ì´ì— ìì„¸ê°€ {state}")
    else:
        print(" - ê°ì§€ëœ ìì„¸ ë³€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\nğŸ‘ï¸ ì‹œì„  ë¶„ì„ ê²°ê³¼:")
    if not_looking_intervals:
        for sec in not_looking_intervals:
            print(f" - {sec}ì´ˆì— ì •ë©´ì„ ë°”ë¼ë³´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        print(" - ëŒ€ë¶€ë¶„ ì •ë©´ì„ ë°”ë¼ë³´ì•˜ìŠµë‹ˆë‹¤.")

    smile_duration = sum(e - s for s, e, st in face_intervals if st == "ì›ƒìŒ")
    neutral_duration = duration - smile_duration

    print("\nğŸ“ í‘œì • í”¼ë“œë°±:")
    if smile_duration > neutral_duration:
        print(f" - ì›ƒëŠ” í‘œì •ì´ ë” ë§ì•˜ìŠµë‹ˆë‹¤ ({smile_duration:.1f}ì´ˆ). ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤! ğŸ˜„")
    elif smile_duration == 0:
        print(" - ì›ƒëŠ” í‘œì •ì´ í•œ ë²ˆë„ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢€ ë” ë¯¸ì†Œë¥¼ ì§€ì–´ë³´ì„¸ìš”. ğŸ™‚")
    else:
        print(f" - ë¬´í‘œì •ì´ ë” ë§ì•˜ìŠµë‹ˆë‹¤ ({neutral_duration:.1f}ì´ˆ). ì¢€ ë” ì›ƒëŠ” í‘œì •ì„ ìœ ì§€í•´ë³´ì„¸ìš”. ğŸ™‚")

    print("\nğŸ“Œ ìì„¸ í”¼ë“œë°±:")
    if posture_change_count == 0:
        print(" - ìì„¸ ë³€í™”ê°€ ê±°ì˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë§¤ìš° ì•ˆì •ëœ ìì„¸ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤. ğŸ‘")
    elif posture_change_count <= 2:
        print(f" - ìì„¸ ë³€í™”ê°€ {posture_change_count}íšŒ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¹„êµì  ì•ˆì •ì ì¸ ìì„¸ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.")
    else:
        print(f" - ìì„¸ ë³€í™”ê°€ ìì£¼ ë°œìƒí–ˆìŠµë‹ˆë‹¤ (ì´ {posture_change_count}íšŒ). ì•ˆì •ëœ ìì„¸ë¥¼ ìœ ì§€í•´ë³´ì„¸ìš”. ğŸ™‚")

    print("\nğŸ‘ï¸ ì‹œì„  í”¼ë“œë°±:")
    if not_looking_count >= 5:
        print(f" - ì •ë©´ì„ ë°”ë¼ë³´ì§€ ì•Šì€ ê²½ìš°ê°€ {not_looking_count}íšŒ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë¥¼ ë” ì‘ì‹œí•´ì£¼ì„¸ìš”. ğŸ‘€")
    else:
        print(f" - ëŒ€ë¶€ë¶„ ì •ë©´ì„ ì˜ ì‘ì‹œí•˜ì˜€ìŠµë‹ˆë‹¤! ({not_looking_count}íšŒë§Œ ê°ì§€ë¨) ğŸ‘")

    print(f"\nâ±ï¸ Mediapipe ì†Œìš” ì‹œê°„: {round(end_time - start_time, 2)} seconds")

# ì‹¤í–‰
if __name__ == "__main__":
    analyze_video(video_path)
